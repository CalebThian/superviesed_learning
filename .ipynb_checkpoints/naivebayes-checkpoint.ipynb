{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "314775f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7de21f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"..//data//train.csv\"\n",
    "test_path = \"..//data//test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cc9274f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID58593', 0.341731678878033, 0.0, 0.586538461538462, 0, 4076, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 796, 3, 0, 5, 0, 4.6, 3445, 1515, 1475, 1185, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "header,train,d = read_csv(train_path)\n",
    "_,test = read_csv(test_path,test=True,Dict = d)\n",
    "print(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4af28531",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.cat = [4,7,8,9,10,11,12,14,15,16,17,18,19,22,24,30,31,32,33,34,35,36,37,38,39,40,41]\n",
    "        self.num = []\n",
    "        self.e = 2.71828182846\n",
    "        self.pi = 3.14159265359\n",
    "        for i in range(1,43): #ID and is_claim is excluded\n",
    "            if i not in self.cat:\n",
    "                self.num.append(i)\n",
    "        print(\"NaiveBayesClassfier is created\")\n",
    "        \n",
    "    def fit(self,data):\n",
    "        self.data = data\n",
    "        self.posterior = dict()\n",
    "        self.claim = [0,0] #Calculation of is_claim in categorical fit\n",
    "        for d in self.data:\n",
    "            claim = d[43]\n",
    "            self.claim[claim] += 1 \n",
    "        self.categorical_prob()\n",
    "        self.numerical_prob()\n",
    "        \n",
    "    \n",
    "    def categorical_prob(self):\n",
    "        # Initialize\n",
    "        for c in self.cat:\n",
    "            self.posterior[c] = dict()\n",
    "        \n",
    "        for d in self.data:\n",
    "            claim = d[43]\n",
    "            for c in self.cat:\n",
    "                if d[c] not in self.posterior[c].keys():\n",
    "                    self.posterior[c][d[c]]=[0,0]\n",
    "                self.posterior[c][d[c]][claim] += 1\n",
    "        \n",
    "        # Assume all features are independent\n",
    "        # P(x_i|C_j) = count(x_i & C_j)/count(C_j)\n",
    "        for k,v in self.posterior.items():\n",
    "            for x,count in v.items():\n",
    "                count[0] = count[0]/self.claim[0]\n",
    "                count[1] = count[1]/self.claim[1]\n",
    "            #print(k,v)\n",
    "        \n",
    "    def numerical_prob(self):\n",
    "        # Assume all the continuous features are gaussian distribution\n",
    "        # take all the data into a list\n",
    "        subdata = dict()\n",
    "        for c in self.num:\n",
    "            # The first subarray store the data of is_claim=0, the second store the data of is_claim=1\n",
    "            subdata[c] = [[0],[0]] # The first element of each subarray is the sum of the rest elements\n",
    "\n",
    "        for d in self.data:\n",
    "            for c in self.num:\n",
    "                claim = d[43]\n",
    "                subdata[c][claim][0] += d[c]\n",
    "                subdata[c][claim].append(d[c])\n",
    "\n",
    "        # Calculate mean and variance\n",
    "        for c in self.num:\n",
    "            self.posterior[c] = [[0,1],[0,1]] # mean and variance of is_claim = 0, mean and variance of is_claim = 1\n",
    "            for i in range(2):\n",
    "                mean = subdata[c][i][0]/(len(subdata[c][i])-1)\n",
    "                variance = 0\n",
    "                for j in range(1,len(subdata[c][i])):\n",
    "                    variance += ((subdata[c][i][j]-mean)**2)\n",
    "                variance /= (len(subdata[c][i])-1)\n",
    "                self.posterior[c][i] = [mean,variance]\n",
    "                \n",
    "    \n",
    "    def predict_one(self,data):\n",
    "        p0 = 1\n",
    "        p1 = 1\n",
    "        for c in self.cat:\n",
    "            p0 *= self.posterior[c][data[c]][0]\n",
    "            p1 *= self.posterior[c][data[c]][1]\n",
    "        \n",
    "        for n in self.num:\n",
    "            mean0 = self.posterior[c][0][0]\n",
    "            var0 = self.posterior[c][0][1]\n",
    "            mean1 = self.posterior[c][1][0]\n",
    "            var1 = self.posterior[c][1][1]\n",
    "            p0 *= (self.e**(-((data[c]-mean0)**2)/2/var0)/(2*self.pi*var0)**0.5)\n",
    "            p1 *= (self.e**(-((data[c]-mean1)**2)/2/var1)/(2*self.pi*var1)**0.5)\n",
    "            #if p0>p1:\n",
    "            #    cmp = '>'\n",
    "            #else:\n",
    "            #    cmp = '<'\n",
    "            #print(f\"{p0}{cmp}{p1}\")\n",
    "        \n",
    "        p0 *= self.claim[0]/(self.claim[0]+self.claim[1])\n",
    "        p1 *= self.claim[1]/(self.claim[0]+self.claim[1])\n",
    "        if p0>p1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    def predict(self,data):\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for d in data:\n",
    "            y_true.append(d[43])\n",
    "            y_pred.append(self.predict_one(d))\n",
    "        \n",
    "        TP = 0\n",
    "        TN = 0\n",
    "        FP = 0\n",
    "        FN = 0\n",
    "        for i,j in zip(y_true,y_pred):\n",
    "            if i==j:\n",
    "                if i == 1:\n",
    "                    TP += 1\n",
    "                else:\n",
    "                    TN += 1\n",
    "            else:\n",
    "                if j == 1:\n",
    "                    FP += 1\n",
    "                else:\n",
    "                    FN += 1\n",
    "        print(TP,TN,FP,FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8e8d7221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayesClassfier is created\n",
      "3733 348 54496 15\n"
     ]
    }
   ],
   "source": [
    "clf = NaiveBayesClassifier()\n",
    "clf.fit(train)\n",
    "clf.predict(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2f5167d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12438917657355408<0.13874066168623267\n",
      "0.0025765098203551426<0.0027022594191822265\n",
      "5.3368010282317886e-05>5.263205378876802e-05\n",
      "1.8581105614850484e-05>1.734273917532778e-05\n",
      "3.848759386344933e-07>3.3778547486630946e-07\n",
      "7.972049199343309e-09>6.579066079306455e-09\n",
      "1.6512741394599226e-10>1.2814082811882904e-10\n",
      "1.133286387011733e-10>8.721644944800771e-11\n",
      "4.4815686604752506e-11>3.2066239951802195e-11\n",
      "3.409715110051251e-11>2.4357680134146438e-11\n",
      "3.2701408554005126e-11>2.3525827664250295e-11\n",
      "1.9923507866376035e-11>1.437410494960864e-11\n",
      "1.5158416832110582e-11>1.0918643754411899e-11\n",
      "9.87906230089214e-12>7.102362186034207e-12\n",
      "2.0462794059174153e-13>1.3833309487206434e-13\n",
      "8.644937246573283e-14>5.5141313697669187e-14\n",
      "6.145591374979927e-14>3.892847279723391e-14\n",
      "4.36883371978188e-14>2.7482587785880716e-14\n",
      "2.8405703744019776e-14>1.7730228726323257e-14\n",
      "1.2846376448889987e-14>7.62570136254885e-15\n",
      "3.552405827872977e-15>2.0162940368959206e-15\n",
      "9.823460503523005e-16>5.331236367566322e-16\n",
      "2.0347624411060708e-17>1.0383678090510714e-17\n",
      "8.467170233922169e-18>4.0975079764848845e-18\n",
      "5.255623388213451e-18>2.478401969768205e-18\n",
      "1.453336450762986e-18>6.553085250907928e-19\n",
      "1.4441146382608796e-18>6.5268589225291605e-19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 22\n",
    "clf.predict_one(train[t]),train[t][43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb19108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(x,base):\n",
    "    result = ln(x)/ln(base)\n",
    "    return result\n",
    "\n",
    "def ln(x):\n",
    "    n = 100000.0\n",
    "    return n * ((x ** (1/n)) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba0e689",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
